# -*- coding: utf-8 -*-
"""AdaIN3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18bWcdbIl9HdyB4lEMHgqrW4s31S7YK7l
"""
"""
# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/bshillingford/python-torchfile.git
# %cd python-torchfile/
!python setup.py install
# %cd ..

!wget -c https://s3.amazonaws.com/xunhuang-public/adain/vgg_normalised.t7
"""
import tensorflow as tf
import numpy as np
import torchfile
import PIL
import time

"""#Global"""

BATCH_SIZE = 8
IMAGE_SIZE = [256, 256, 3]
LEARNING_RATE = 1e-4
LR_DECAY_RATE = 5e-5
DECAY_STEPS = 1.0
"""#Utils

"""
def deprocess(image, mode='BGR'):
    if mode == 'BGR':
        return image + np.array([103.939, 116.779, 123.68])
    else:
        return image + np.array([123.68, 116.779, 103.939])

def load_and_process_img(path_to_img):
  """Load the image and preprocess according to trained VGG19 model standards.
  """
  new_min_dim = 512
  img = tf.io.read_file(path_to_img)
  # This creates RGB image
  try:
    img = tf.image.decode_jpeg(img, channels=3)
  except:
    return None

  # Scale minimum dimension to 512px
  height = tf.cast(tf.shape(img)[0], tf.float32)
  width = tf.cast(tf.shape(img)[1], tf.float32)
  min_dim = tf.minimum(height, width)
  scale = new_min_dim / min_dim
  img = tf.image.resize(img, (scale*height, scale*width))

  # This scales pixel values and reorders channels to BGR
  #img = tf.keras.applications.vgg19.preprocess_input(img)
  # img = tf.image.convert_image_dtype(img, tf.float32)
  # img = tf.cast(img, tf.float32)
  #img /= 255.

  crop_size = 256
  img = tf.image.random_crop(img, (crop_size, crop_size, 3))

  return img

def preprocess_img(img):
  """Preprocess image."""
  crop_size = 256
  img = tf.image.random_crop(img, (crop_size, crop_size, 3))

  return img


def process_path(path_to_img):
  img = load_and_process_img(path_to_img)
  img = preprocess_img(img)
  return img

def load_img(path_to_img):
    img = tf.io.read_file(path_to_img)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)

    min_dim = 512

    shape = tf.cast(tf.shape(img)[:-1], tf.float32)
    small_dim = tf.reduce_min(shape)
    scale = min_dim / small_dim
    new_shape = tf.cast(shape * scale, tf.int32)

    img = tf.image.resize(img, new_shape)
    img = tf.image.random_crop(img, [256,256,3])
    #img = img[tf.newaxis, :]
    

    return img
    
def tensor_to_image(tensor):
  tensor = tensor*255
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor)>3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return PIL.Image.fromarray(tensor)

def prepare_dataset(path_to_imgs):
  dataset = tf.data.Dataset.list_files(path_to_imgs)
  dataset = dataset.map(load_and_process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
  return dataset

def load_img_Inference(path_to_img, max_dim=None, resize=True, frame = False):

    img = tf.io.read_file(path_to_img)     
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)

    if resize:
        if frame:
            new_shape = tf.cast([218, 512], tf.int32)
            img = tf.image.resize(img, new_shape)
        else:  
            new_shape = tf.cast([256, 256], tf.int32)
            img = tf.image.resize(img, new_shape)

    if max_dim:
        shape = tf.cast(tf.shape(img)[:-1], tf.float32)
        long_dim = max(shape)
        scale = max_dim / long_dim
        new_shape = tf.cast(shape * scale, tf.int32)
        img = tf.image.resize(img, new_shape)
        
    img = img[tf.newaxis, :]


    return img


def decode_img(img, reverse_channels=False):
  """Decodes preprocessed images."""

  # perform the inverse of the preprocessiing step
  img *= 255.
  if reverse_channels:
    img = img[..., ::-1]

  img = tf.cast(img, dtype=tf.uint8)
  return img

style_path ="/home/litsos/style_transfer/dataset/wikiart" #"/home/michlist/Desktop/Style_Transfer/Tensorflow2_Style_Transfer/dataset"
content_path = "/home/litsos/style_transfer/dataset/train2014"

"""##Dataset"""

style_train_ds = prepare_dataset(style_path + '/**/*.jpg')

content_train_ds = prepare_dataset(content_path + '/*.jpg')

print(f"Style images {len(style_train_ds)}")

print(f"Contain images {len(content_train_ds)}")

train_ds = tf.data.Dataset.zip((style_train_ds, content_train_ds))
train_ds = train_ds.shuffle(BATCH_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)

print(f"Final train dataset {len(train_ds)}")



"""#Networks"""
def get_encoder_from_torch(target_layer='relu4_1'):
  """Load a model from t7 and translate it to tensorflow."""
  t7 = torchfile.load('./vgg_normalised.t7', force_8bytes_long=True)

  inputs = tf.keras.Input((None, None, 3), name="vgg_input")

  x = inputs
    
  style_outputs = []
  content_outputs = []
  for idx,module in enumerate(t7.modules):
    name = module.name.decode() if module.name is not None else None
    
    if idx == 0:
      name = 'preprocess'  # VGG 1st layer preprocesses with a 1x1 conv to multiply by 255 and subtract BGR mean as bias

    if module._typename == b'nn.SpatialReflectionPadding':
      x = tf.keras.layers.Lambda(
          lambda t: tf.pad(t, [[0, 0], [1, 1], [1, 1], [0, 0]],
          mode='REFLECT'))(x)            
    elif module._typename == b'nn.SpatialConvolution':
      filters = module.nOutputPlane
      kernel_size = module.kH
      weight = module.weight.transpose([2,3,1,0])
      weight = tf.cast(weight, tf.float32)
      bias = module.bias
      x = tf.keras.layers.Conv2D(filters, kernel_size, padding='valid', activation='relu', name=name,
                    kernel_initializer=tf.keras.initializers.Constant(weight),
                    bias_initializer=tf.keras.initializers.Constant(bias),
                    trainable=False)(x)
      if name in style_layers:
        style_outputs.append(x)
      if name in content_layers:
        content_outputs.append(x)
    elif module._typename == b'nn.ReLU':
      pass # x = layers.Activation('relu', name=name)(x)
    elif module._typename == b'nn.SpatialMaxPooling':
      x = tf.keras.layers.MaxPooling2D(padding='same', name=name)(x)
    else:
      raise NotImplementedError(module._typename)

    if name == target_layer:
      # print("Reached target layer", target_layer)
      break
  
  # Get output layers corresponding to style and content layers 
  #style_outputs = [vgg.get_layer(name).output for name in style_layers]
  #content_outputs = [vgg.get_layer(name).output for name in content_layers]
  model_outputs = style_outputs + content_outputs

  return tf.keras.models.Model(inputs=inputs, outputs=model_outputs)

def adaptive_instance_normalization(style, content, epsilon=1e-5):
  style_mean, style_var = tf.nn.moments(style, [1,2], keepdims=True)
  style_std = tf.sqrt(style_var + epsilon)

  content_mean, content_var = tf.nn.moments(content, [1,2], keepdims=True)
  content_std = tf.sqrt(content_var + epsilon)

  adain = (style_std*(content - content_mean)/content_std) + style_mean  
  return adain

def get_decoder(encoder):
  """Creates a trainable decoder, that mirrors the encoder.

  Pooling layers are replaced with nearest up-sampling layers and reflection
  padding is used to avoid border artifacts.
  """
  decoder = tf.keras.Sequential()
  
  inputs = tf.keras.Input((None, None, encoder.layers[-1].filters))
  # Mirror the encoder
  x = inputs
  for i in reversed(range(4,len(encoder.layers))):
    layer = encoder.layers[i]
    if isinstance(layer, tf.keras.layers.MaxPooling2D):
      x = tf.keras.layers.UpSampling2D()(x)
    elif isinstance(layer, tf.keras.layers.Conv2D):
      x = tf.keras.layers.Conv2D(
          layer.get_weights()[0].shape[2], 
          layer.kernel_size, 
          activation=tf.keras.activations.relu)(tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]],
          mode='REFLECT'))

  # Finally reduce number of channels to three
  x = tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]],
           mode='REFLECT')
  x = tf.keras.layers.Conv2D(3, 
                             3)(x) 
                             # activation=tf.keras.activations.relu)(x)
  outputs = x
    
  return tf.keras.models.Model(inputs, outputs)



"""#Losses"""

def get_content_loss(adain_output, target_encoded):
  return tf.reduce_sum(tf.reduce_mean(tf.square(adain_output - target_encoded), axis=[1,2]))

def get_style_loss(base_style_encoded, target_encoded):
  eps = 1e-5
  
  base_style_mean, base_style_var = tf.nn.moments(base_style_encoded, 
                                                  axes=[1,2])
  # Add epsilon for numerical stability for gradients close to zero
  base_style_std = tf.math.sqrt(base_style_var + eps)

  target_mean, target_var = tf.nn.moments(target_encoded,
                                          axes=[1,2])
  # Add epsilon for numerical stability for gradients close to zero
  target_std = tf.math.sqrt(target_var + eps)

  mean_diff = tf.reduce_sum(tf.square(base_style_mean - target_mean)) 
  std_diff = tf.reduce_sum(tf.square(base_style_std - target_std)) 
  return mean_diff + std_diff

STYLE_LOSS_WEIGHT = 1e-4

def get_loss(adain_output, base_style_encoded, target_encoded):
  # Content loss
  content_loss = get_content_loss(adain_output, target_encoded[-1])
  
  # Style loss
  style_loss = 0
  for i in range(num_style_layers):
    style_loss += get_style_loss(base_style_encoded[i], target_encoded[i])

  return content_loss + STYLE_LOSS_WEIGHT * style_loss

"""#Train Step"""

@tf.function
def train_step(content_img, style_img):
  with tf.GradientTape() as tape:
    encoded_content = encoder(content_img)
    encoded_style = encoder(style_img)

    adain_output = adaptive_instance_normalization(encoded_style[-1], encoded_content[-1])

    target_img = decoder(adain_output)

    #target_img = deprocess(target_img)
    #target_img = tf.reverse(target_img, axis=[-1])

    #target_img = tf.clip_by_value(target_img, 0.0, 255.0)

    loss = get_loss(adain_output, encoded_style, encoder(target_img))

    

  gradients = tape.gradient(loss, decoder.trainable_variables)
  optimizer.apply_gradients(zip(gradients, decoder.trainable_variables))

  train_loss(loss)

# Content layer where will pull our feature maps
content_layers = ['conv4_1'] 

# Style layer we are interested in
style_layers = ['conv1_1',
                'conv2_1',
                'conv3_1', 
                'conv4_1' 
               ]

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

encoder = get_encoder_from_torch()
decoder = get_decoder(encoder)


learning_rate = tf.keras.optimizers.schedules.InverseTimeDecay(LEARNING_RATE, DECAY_STEPS, LR_DECAY_RATE)
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

train_loss = tf.keras.metrics.Mean(name='train_loss')

#style_tr, content_tr = next(iter(train_ds))

#encoded_content = encoder(style_tr)
#encoded_style = encoder(content_tr)

#adain_output = adaptive_instance_normalization(encoded_style[-1] , encoded_content[-1])

#target_img = decoder(adain_output)

#target_img = tf.clip_by_value(target_img, 0.0, 255.0)

#encoded_style_loss = loss_net(tf.keras.applications.vgg19.preprocess_input(style_tr))
#encoded_target_loss = loss_net(tf.keras.applications.vgg19.preprocess_input(target_img))

#loss = get_loss(adain_output, encoded_style_loss, encoded_target_loss)

EPOCHS = 16
PROGBAR = tf.keras.utils.Progbar(len(train_ds))
for epoch in range(EPOCHS):
  # Reset the metrics at the start of the next epoch
  train_loss.reset_states()

  step = 0
  start_time = time.perf_counter()
  for (style_tr, content_tr) in train_ds.as_numpy_iterator():
      start_time = time.perf_counter()
      train_step(content_tr, style_tr)
      print(f"Train step: {time.perf_counter() - start_time}")
      # start_time = time.perf_counter()
      step += 1
      PROGBAR.update(step)

  template = 'Epoch {}, Loss: {}'
  print(template.format(epoch+1,
                        train_loss.result()))

decoder.save_weights("./weights/git/decoder")
"""
style = load_and_process_img("/content/drive/MyDrive/content_images/Wikiart/Action_painting/hans-hofmann_the-wind-1942.jpg")
content = load_and_process_img("/content/drive/MyDrive/content_images/mini_batch/000000000802.jpg")
style = style[tf.newaxis, :]
content = content[tf.newaxis, :]
decoder.load_weights("/content/decoder")

encoded_content = encoder(content)
encoded_style = encoder(style)

adain_output = adaptive_instance_normalization(encoded_style[-1], encoded_content[-1])

target_img = decoder(adain_output)
target_img = decode_img(target_img)

tensor_to_image(content)

tensor_to_image(style)

target_img

target_img = tf.clip_by_value(target_img, 0.0, 255.0)
tensor_to_image(target_img)
"""